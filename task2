import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import classification_report, accuracy_score
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
data = pd.read_csv('/content/IRIS.csv')

# Inspect data structure
print("Dataset Information:")
data.info()

# Check for missing values
print("\nMissing Values:")
print(data.isnull().sum())

# Descriptive statistics
print("\nSpecies Value Counts:")
print(data['species'].value_counts())

# Visualizations
sns.set_palette('RdBu')
plt.pie(data['species'].value_counts(), labels=data['species'].unique(), autopct='%1.0f%%', colors=sns.color_palette('RdBu'))
plt.title("Species Distribution")
plt.show()

# Plot average feature values by species
features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
for feature in features:
    data.groupby('species')[feature].mean().plot(kind='bar', color=sns.color_palette('RdBu'))
    plt.title(f"Average {feature.replace('_', ' ').title()} by Species")
    plt.ylabel("Mean")
    plt.xlabel("Species")
    plt.show()

# Encode species column
le = LabelEncoder()
data['species'] = le.fit_transform(data['species'])

# Split features and target
X = data.drop(columns='species', axis=1)
y = data['species']

# Hyperparameter tuning
model_params = {
    'Linear Regression': {
        'model': LinearRegression(),
        'params': {
            'fit_intercept': [True, False],
            'positive': [True, False]
        }
    },
    'Logistic Regression': {
        'model': LogisticRegression(),
        'params': {
            'penalty': ['l1', 'l2'],
            'C': [0.01, 0.1, 1, 10],
            'solver': ['saga'],
            'max_iter': [100, 200, 500],
            'multi_class': ['multinomial']
        }
    },
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'n_estimators': [50, 100, 200],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10]
        }
    }
}

scores = []
for model_name, mp in model_params.items():
    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=True)
    clf.fit(X, y)
    scores.append({
        'Model': model_name,
        'Best Score': clf.best_score_,
        'Best Params': clf.best_params_
    })

scores_df = pd.DataFrame(scores)
print("\nHyperparameter Tuning Results:")
print(scores_df)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Logistic Regression
log_reg = LogisticRegression(C=1, max_iter=100, multi_class='multinomial', solver='saga', random_state=42)
log_reg.fit(X_train, y_train)

# Evaluate the model
train_score = log_reg.score(X_train, y_train)
test_score = log_reg.score(X_test, y_test)
predictions = log_reg.predict(X_test)
report = classification_report(y_test, predictions)

print("\nLogistic Regression Performance:")
print(f"Train Accuracy: {train_score:.2f}")
print(f"Test Accuracy: {test_score:.2f}")
print("\nClassification Report:")
print(report)

# Visualize prediction distribution
sns.histplot(predictions, kde=False, bins=3, color='skyblue')
plt.title("Prediction Distribution")
plt.xlabel("Predicted Class")
plt.ylabel("Frequency")
plt.show()
